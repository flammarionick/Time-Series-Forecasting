{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96e9227",
   "metadata": {},
   "source": [
    "\n",
    "# Air Quality Forecasting — Rubric-Ready Notebook\n",
    "\n",
    "**Course:** Machine Learning Techniques I  \n",
    "**Student:** _<Your Name Here>_  \n",
    "**Competition:** _<Kaggle link here>_\n",
    "\n",
    "This notebook is a clean, self-contained version of the starter, modified to meet every rubric item:\n",
    "- Approach & rationale for RNN/LSTM\n",
    "- Data exploration, preprocessing, and feature engineering (with explanations)\n",
    "- Model design details and justification\n",
    "- ≥ 15 experiments with varied hyperparameters (automatic table + discussion prompts)\n",
    "- Results & Discussion with RMSE formula, plots, and error analysis\n",
    "- Kaggle submission cell\n",
    "- Conclusion & IEEE-style references\n",
    "\n",
    "> Write your own commentary in the marked cells to keep originality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c32982",
   "metadata": {},
   "source": [
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, glob, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show versions (helpful for reproducibility)\n",
    "import sklearn, platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "\n",
    "# Optional: enable TF logs as needed\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow:\", tf.__version__)\n",
    "    print(\"GPU:\", tf.config.list_physical_devices('GPU'))\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available in this environment:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd82ac3",
   "metadata": {},
   "source": [
    "## 1. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try Kaggle input paths, else fall back to local data/\n",
    "def find_file(name_hint):\n",
    "    # Search Kaggle input\n",
    "    for p in glob.glob(\"/kaggle/input/**/*\", recursive=True):\n",
    "        if p.lower().endswith(name_hint):\n",
    "            return p\n",
    "    # Fallback: local working dir\n",
    "    for p in glob.glob(f\"./**/*{name_hint}\", recursive=True):\n",
    "        return p\n",
    "    return None\n",
    "\n",
    "train_path = find_file(\"train.csv\")\n",
    "test_path  = find_file(\"test.csv\")\n",
    "sample_path= find_file(\"sample_submission.csv\")\n",
    "\n",
    "print(\"train:\", train_path)\n",
    "print(\"test :\", test_path)\n",
    "print(\"sample:\", sample_path)\n",
    "assert train_path and test_path and sample_path, \"Could not locate train/test/sample_submission CSVs.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1fedf4",
   "metadata": {},
   "source": [
    "## 2. Utilities (Loading, Features, Windowing, Scaling, Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4434fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(y, yhat): \n",
    "    return sqrt(mean_squared_error(y, yhat))\n",
    "\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    # Build or parse datetime\n",
    "    if 'datetime' not in df.columns:\n",
    "        if set(['year','month','day','hour']).issubset(df.columns):\n",
    "            df['datetime'] = pd.to_datetime(dict(year=df['year'], month=df['month'], day=df['day'], hour=df['hour']))\n",
    "        else:\n",
    "            for c in df.columns:\n",
    "                if any(k in c.lower() for k in ['date','time','datetime']):\n",
    "                    try:\n",
    "                        df['datetime'] = pd.to_datetime(df[c])\n",
    "                        break\n",
    "                    except: \n",
    "                        pass\n",
    "            if 'datetime' not in df.columns:\n",
    "                raise ValueError(\"Need year/month/day/hour or a parseable datetime column.\")\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    d = df.copy()\n",
    "\n",
    "    # Robust target selection\n",
    "    target = 'PM2.5' if 'PM2.5' in d.columns else ('pm2.5' if 'pm2.5' in d.columns else None)\n",
    "    assert target is not None, \"Expected PM2.5/pm2.5 in the data\"\n",
    "\n",
    "    # One-hot any object columns (if any)\n",
    "    cat_cols = [c for c in d.columns if d[c].dtype == 'object' and c != 'datetime']\n",
    "    if cat_cols:\n",
    "        d = pd.get_dummies(d, columns=cat_cols, dummy_na=False)\n",
    "\n",
    "    # Seasonal encodings\n",
    "    d['hour']      = d['datetime'].dt.hour\n",
    "    d['dayofweek'] = d['datetime'].dt.dayofweek\n",
    "    d['month']     = d['datetime'].dt.month\n",
    "    for k, m in [('hour',24), ('dayofweek',7), ('month',12)]:\n",
    "        d[f'{k}_sin'] = np.sin(2*np.pi*d[k]/m)\n",
    "        d[f'{k}_cos'] = np.cos(2*np.pi*d[k]/m)\n",
    "\n",
    "    # Lags & rolling on target (shift to avoid leakage)\n",
    "    for L in [1,3,6,12,24]:\n",
    "        d[f'target_lag{L}'] = d[target].shift(L)\n",
    "    d['target_roll6']  = d[target].shift(1).rolling(6).mean()\n",
    "    d['target_roll24'] = d[target].shift(1).rolling(24).mean()\n",
    "    d['target_std24']  = d[target].shift(1).rolling(24).std()\n",
    "\n",
    "    # Interpolate numeric features only\n",
    "    num_cols = [c for c in d.columns if c != 'datetime']\n",
    "    d[num_cols] = d[num_cols].interpolate(limit_direction='both')\n",
    "    return d\n",
    "\n",
    "def time_split(df, val_frac=0.15):\n",
    "    n = len(df); s = int(n*(1-val_frac))\n",
    "    return df.iloc[:s].copy(), df.iloc[s:].copy()\n",
    "\n",
    "def make_windows(df, feat_cols, target_col, window=48):\n",
    "    dd = df.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    Xf = dd[feat_cols].values\n",
    "    y  = dd[target_col].values\n",
    "    X, Y = [], []\n",
    "    for i in range(window, len(dd)):\n",
    "        X.append(Xf[i-window:i, :]); Y.append(y[i])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def scale_by_train(Xtr, Xva):\n",
    "    sc = StandardScaler()\n",
    "    nT, W, F = Xtr.shape\n",
    "    Xtr2 = sc.fit_transform(Xtr.reshape(nT*W, F)).reshape(nT, W, F)\n",
    "    nV, Wv, Fv = Xva.shape\n",
    "    Xva2 = sc.transform(Xva.reshape(nV*Wv, Fv)).reshape(nV, Wv, Fv)\n",
    "    return Xtr2, Xva2, sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0aa4c",
   "metadata": {},
   "source": [
    "## 3. Data Exploration (add your commentary below each plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c94de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = load_data(train_path)\n",
    "test  = load_data(test_path)\n",
    "\n",
    "# Determine correct target name\n",
    "TARGET = 'PM2.5' if 'PM2.5' in train.columns else ('pm2.5' if 'pm2.5' in train.columns else None)\n",
    "assert TARGET is not None, \"Target column PM2.5/pm2.5 not found\"\n",
    "\n",
    "print(\"Rows in train:\", len(train), \" / test:\", len(test))\n",
    "print(\"Columns:\", list(train.columns))\n",
    "print(train[[TARGET]].describe())\n",
    "\n",
    "# Simple line plot (first 3000 points) — you explain seasonality/spikes below\n",
    "plt.figure()\n",
    "plt.plot(train['datetime'].iloc[:3000], train[TARGET].iloc[:3000])\n",
    "plt.title(\"PM2.5 over time (first ~3000 hours)\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(TARGET); plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309b6f4c",
   "metadata": {},
   "source": [
    "\n",
    "**Explain (in your own words):**  \n",
    "- Visible trends/seasonality, winter spikes, any missingness.  \n",
    "- Why these patterns motivate RNN/LSTM (temporal dependencies).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1f3c5",
   "metadata": {},
   "source": [
    "## 4. Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_fe = engineer_features(train)\n",
    "test_fe  = engineer_features(test)\n",
    "\n",
    "feat_cols = [c for c in train_fe.columns if c not in ['datetime', TARGET]]\n",
    "tr, va = time_split(train_fe, val_frac=0.15)\n",
    "\n",
    "W = 48\n",
    "Xtr, Ytr = make_windows(tr, feat_cols, target_col=TARGET, window=W)\n",
    "Xva, Yva = make_windows(va, feat_cols, target_col=TARGET, window=W)\n",
    "Xtr2, Xva2, scaler = scale_by_train(Xtr, Xva)\n",
    "\n",
    "Xtr2.shape, Xva2.shape, len(feat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8aaa4",
   "metadata": {},
   "source": [
    "\n",
    "**Explain (in your own words):**  \n",
    "- What each engineered feature adds (lags, rolling, seasonality).  \n",
    "- How leakage is avoided (shifts, time-based split).  \n",
    "- Why scaling helps optimization.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298709a",
   "metadata": {},
   "source": [
    "## 5. Model Design & Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55541562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "def build_lstm(input_shape, units1=128, units2=64, dropout=0.2, lr=1e-3, optimizer_name='adam', clipnorm=1.0):\n",
    "    m = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(units1, return_sequences=True),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.LSTM(units2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    if optimizer_name.lower()=='adamw':\n",
    "        opt = optimizers.AdamW(learning_rate=lr)\n",
    "    elif optimizer_name.lower()=='rmsprop':\n",
    "        opt = optimizers.RMSprop(learning_rate=lr, clipnorm=clipnorm)\n",
    "    else:\n",
    "        opt = optimizers.Adam(learning_rate=lr, clipnorm=clipnorm)\n",
    "    m.compile(optimizer=opt, loss='mse')\n",
    "    return m\n",
    "\n",
    "model = build_lstm(Xtr2.shape[1:], 128, 64, dropout=0.2, lr=1e-3, optimizer_name='adam', clipnorm=1.0)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe0b8f8",
   "metadata": {},
   "source": [
    "\n",
    "**Justify (in your own words):**  \n",
    "- Why LSTM (gating mitigates vanishing gradients), layers/units, dropout, optimizer, gradient clipping.  \n",
    "- Loss choice (MSE) and why RMSE is the leaderboard metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea8da0",
   "metadata": {},
   "source": [
    "## 6. First Run (baseline) + Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de99eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "hist = model.fit(Xtr2, Ytr, validation_data=(Xva2, Yva), epochs=60, batch_size=64, callbacks=[es], verbose=0)\n",
    "\n",
    "# Learning curve\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='val')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MSE'); plt.title('Learning Curve'); plt.legend(); plt.tight_layout()\n",
    "\n",
    "# Validation RMSE\n",
    "pred = model.predict(Xva2, verbose=0).reshape(-1)\n",
    "val_rmse = rmse(Yva, pred)\n",
    "print(\"Validation RMSE:\", round(val_rmse, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163a2ef",
   "metadata": {},
   "source": [
    "## 7. Required Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (i) Predictions vs Actual for a validation week\n",
    "k = min(168, len(Yva))\n",
    "plt.figure()\n",
    "plt.plot(range(k), Yva[-k:], label='Actual')\n",
    "plt.plot(range(k), pred[-k:], label='Predicted')\n",
    "plt.title(f'Validation Week — RMSE {val_rmse:.2f}')\n",
    "plt.legend(); plt.xlabel('Hour'); plt.ylabel(TARGET); plt.tight_layout()\n",
    "\n",
    "# (ii) Residuals histogram\n",
    "res = Yva - pred\n",
    "plt.figure()\n",
    "plt.hist(res, bins=40)\n",
    "plt.title('Residuals Histogram'); plt.xlabel('Error'); plt.ylabel('Count'); plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428dac41",
   "metadata": {},
   "source": [
    "\n",
    "**Discuss (in your own words):**  \n",
    "- Over/under-fitting signals, residual patterns, winter spikes, bias corrections.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde543a",
   "metadata": {},
   "source": [
    "## 8. Experiments (≥ 15 configs) — auto table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools, pandas as pd\n",
    "\n",
    "configs = [\n",
    "    (\"01\",\"lstm\",24,  64,  0,  0.0,\"adam\", 1e-3,64),\n",
    "    (\"02\",\"lstm\",48, 128, 64, 0.2,\"adam\", 1e-3,64),\n",
    "    (\"03\",\"gru\", 48, 128, 64, 0.2,\"adam\", 5e-4,64),\n",
    "    (\"04\",\"lstm\",72, 128, 64, 0.3,\"adam\", 1e-3,128),\n",
    "    (\"05\",\"lstm\",48, 256, 64, 0.2,\"adam\", 1e-3,64),\n",
    "    (\"06\",\"lstm\",48, 128, 64, 0.2,\"adam\", 3e-4,64),\n",
    "    (\"07\",\"lstm\",48, 128, 64, 0.2,\"adamw\",1e-3,64),\n",
    "    (\"08\",\"lstm\",48, 128, 32, 0.2,\"adam\", 1e-3,64),\n",
    "    (\"09\",\"lstm\",36, 128, 64, 0.2,\"adam\", 1e-3,64),\n",
    "    (\"10\",\"lstm\",48,128, 64, 0.5,\"adam\", 1e-3,64),\n",
    "    (\"11\",\"lstm\",48,128, 64, 0.2,\"rmsprop\",1e-3,64),\n",
    "    (\"12\",\"lstm\",48, 96,  48, 0.2,\"adam\", 1e-3,256),\n",
    "    (\"13\",\"lstm\",48,128, 64, 0.2,\"adam\", 1e-4,64),\n",
    "    (\"14\",\"gru\", 72,128, 64, 0.2,\"adam\", 1e-3,64),\n",
    "    (\"15\",\"lstm\",48,128, 64, 0.2,\"adam\", 1e-3,64),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for (ID, arch, W_, u1, u2, dr, opt, lr_, bs) in configs:\n",
    "    # rebuild windows for W_\n",
    "    Xtr_, Ytr_ = make_windows(tr, feat_cols, target_col=TARGET, window=W_)\n",
    "    Xva_, Yva_ = make_windows(va, feat_cols, target_col=TARGET, window=W_)\n",
    "    Xtr_, Xva_, sc_ = scale_by_train(Xtr_, Xva_)\n",
    "\n",
    "    # model\n",
    "    if arch=='gru':\n",
    "        m = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(W_, len(feat_cols))),\n",
    "            tf.keras.layers.GRU(u1, return_sequences=True),\n",
    "            tf.keras.layers.Dropout(dr),\n",
    "            tf.keras.layers.GRU(u2) if u2>0 else tf.keras.layers.GRU(u1),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "    else:\n",
    "        m = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(W_, len(feat_cols))),\n",
    "            tf.keras.layers.LSTM(u1, return_sequences=True),\n",
    "            tf.keras.layers.Dropout(dr),\n",
    "            tf.keras.layers.LSTM(u2) if u2>0 else tf.keras.layers.LSTM(u1),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "    if opt=='adamw':\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_)\n",
    "    elif opt=='rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr_, clipnorm=1.0)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_, clipnorm=1.0)\n",
    "\n",
    "    m.compile(optimizer=optimizer, loss='mse')\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    m.fit(Xtr_, Ytr_, validation_data=(Xva_, Yva_), epochs=60, batch_size=bs, callbacks=[es], verbose=0)\n",
    "    pv = m.predict(Xva_, verbose=0).reshape(-1)\n",
    "    score = rmse(Yva_, pv)\n",
    "    rows.append({\n",
    "        \"ID\":ID,\"Arch\":(\"GRU×2\" if arch=='gru' else \"LSTM×2\"),\n",
    "        \"Window W\":W_, \"Units (L1/L2)\":f\"{u1}/{u2 if u2>0 else '—'}\",\n",
    "        \"Dropout\":dr, \"Optim\":opt, \"LR\":lr_, \"Batch\":bs,\n",
    "        \"Features (extra)\":\"all feats\",\"Grad Clip\":\"clipnorm=1.0\",\n",
    "        \"Val RMSE\":round(score,3), \"Kaggle Public\":\"\", \"Kaggle Private\":\"\"\n",
    "    })\n",
    "\n",
    "exp = pd.DataFrame(rows)\n",
    "exp = exp.sort_values(\"ID\")\n",
    "exp.to_csv(\"experiments.csv\", index=False)\n",
    "exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e7148",
   "metadata": {},
   "source": [
    "\n",
    "**Discuss (in your own words):**  \n",
    "- Which changes helped/hurt (window size, dropout, optimizer, units)?  \n",
    "- Possible reasons (capacity vs overfit, Adam vs AdamW, larger W for winter spikes, etc.).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f665b00f",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Results & Discussion\n",
    "\n",
    "**RMSE definition:**  \n",
    "\\[\n",
    "\\mathrm{RMSE}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^2}.\n",
    "\\]\n",
    "\n",
    "Summarize trends across experiments: where did RMSE improve, and why? Mention RNN challenges (vanishing/exploding gradients) and how dropout + clipnorm and LSTM gating help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4bdd5",
   "metadata": {},
   "source": [
    "## 10. Kaggle Submission (train on all data, predict test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train on full train with the best config you observed (edit as needed)\n",
    "BEST_W, BEST_U1, BEST_U2, BEST_DROPOUT, BEST_LR, BEST_OPT, BEST_BATCH = 48, 128, 64, 0.2, 1e-3, 'adam', 64\n",
    "\n",
    "# Rebuild with full train features\n",
    "train_fe = engineer_features(train)\n",
    "test_fe  = engineer_features(test)\n",
    "feat_cols = [c for c in train_fe.columns if c not in ['datetime', TARGET]]\n",
    "\n",
    "# Windows for full train (to fit scaler + model)\n",
    "Xfull, Yfull = make_windows(train_fe, feat_cols, target_col=TARGET, window=BEST_W)\n",
    "Xfull2, _, sc_full = scale_by_train(Xfull, Xfull[:1])\n",
    "\n",
    "model_full = build_lstm((BEST_W, len(feat_cols)), BEST_U1, BEST_U2, dropout=BEST_DROPOUT, lr=BEST_LR, optimizer_name=BEST_OPT, clipnorm=1.0)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "model_full.fit(Xfull2, Yfull, epochs=50, batch_size=BEST_BATCH, callbacks=[es], verbose=0)\n",
    "\n",
    "# Build context windows over concatenated train+test\n",
    "cat = pd.concat([train_fe, test_fe], ignore_index=True)\n",
    "def make_windows_no_target(df, feat_cols, window):\n",
    "    Xf = df[feat_cols].values\n",
    "    X = []\n",
    "    for i in range(window, len(df)):\n",
    "        X.append(Xf[i-window:i, :])\n",
    "    return np.array(X)\n",
    "\n",
    "Xcat = make_windows_no_target(cat, feat_cols, BEST_W)\n",
    "Xcat2 = sc_full.transform(Xcat.reshape(Xcat.shape[0]*Xcat.shape[1], Xcat.shape[2])).reshape(Xcat.shape)\n",
    "\n",
    "preds_all = model_full.predict(Xcat2, verbose=0).reshape(-1)\n",
    "preds = preds_all[-len(test):]  # last |test| predictions\n",
    "\n",
    "# Write submission using sample's column name\n",
    "sample = pd.read_csv(sample_path)\n",
    "submit_col = sample.columns[-1]\n",
    "sample[submit_col] = preds[:len(sample)]\n",
    "sample.to_csv(\"submission.csv\", index=False)  # Kaggle picks this automatically\n",
    "print(\"Wrote submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfd296",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Conclusion\n",
    "Restate the problem, summarize your best approach & score, and list concrete next steps (multi-horizon forecasts, TCN/attention, holiday/traffic features, quantile loss for uncertainty).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a69db2",
   "metadata": {},
   "source": [
    "\n",
    "## 12. References (IEEE style)\n",
    "[1] S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” *Neural Computation*, 1997.  \n",
    "[2] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies with gradient descent is difficult,” *IEEE Trans. Neural Networks*, 1994.  \n",
    "[3] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*, MIT Press, 2016.  \n",
    "[4] G. E. P. Box, G. M. Jenkins, and G. C. Reinsel, *Time Series Analysis: Forecasting and Control*, 4th ed., 2008.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
